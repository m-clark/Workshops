---
title: "data exploration case studies"
output: html_notebook
---


# Medicare payment database - subset by state

This notebook can be used to subset the 2014 medicare provider utilization and payment data by state.

The raw data are available [here](http://www.cms.gov/Research-Statistics-Data-and-Systems/Statistics-Trends-and-Reports/Medicare-Provider-Charge-Data/Physician-and-Other-Supplier.html)



This is a rather large file, almost .5Gb in zipped form and over 9 million records.  Using read_tsv (for tab separated values) will produce a roughly 1.8 Gb data frame.

```{r, eval=FALSE}
library(feather)
cms <- readr::read_tsv('data/Medicare_Provider_Util_Payment_PUF_CY2014.txt')
# write_feather(cms, 'data/Medicare_Provider_Util_Payment_PUF_CY2014.feather')
# cms = read_feather('data/Medicare_Provider_Util_Payment_PUF_CY2014.feather')
```

For our purposes we will use the state of Florida (and later Michigan), but first I will note a couple things. Using <span class="func"></span>read_tsv creates an object that is both  <span class="objclass">data.frame</span> and a 'tibble' or  <span class="objclass">tbl</span> object.  Using  <span class="pack">dplyr</span> with such an object will allow for faster operations on larger data.  What you don't want to use are things like  <span class="func">tapply</span> or  <span class="func">aggregate</span>. The former will almost always require additional processing, and the latter is extremely slow.  In addition, if you need additional speed and memory management you can use the  <span class="pack">data.table</span> package. In the following I demonstrate both reading the data and using a group by operation to get the mean charge amount.

```{r, message=FALSE, warning=FALSE}
library(dplyr)
system.time({
  cms <- readr::read_tsv('data/Medicare_Provider_Util_Payment_PUF_CY2014.txt', progress=F)
})
```

Now the group operation.

```{r}
system.time({
cms %>% 
  group_by(nppes_provider_state) %>% 
  summarise(meanCharge = mean(average_submitted_chrg_amt, na.rm=T))
})
```


So around a half second for the grouped summary.  Now we'll do the same with <span class="pack">data.table</span>.

```{r, warning=FALSE, message=FALSE}
library(data.table)
system.time({
  cms = fread('data/Medicare_Provider_Util_Payment_PUF_CY2014.txt', verbose=F, showProgress=F)
})
```


```{r}
system.time({
  cms[,list(meanCharge = mean(average_submitted_chrg_amt, na.rm=T)), by=nppes_provider_state]
})
```

You can ignore the difference in read times. Some times read_tsv was faster, sometimes fread was (the former seemed notably variable). But both were far faster than base R's <span class="func">read.delim</span> (about 4 min) and made better use of memory.  The <span class="objclass">data.table</span> object resulted in a faster grouped operation, and this is to be expected.  The basic trade-off between the two is speed gain for legibility/ease of use.  If you're not using a large data set, <span class="pack">data.table</span> doesn't have much to offer (my opinion!), and if you do, it is probably the clear choice for many operations.  The nice thing is *you don't have to choose*! Both produce objects that are also of class data.frame and so you can use dplyr operations on data.table objects (though the reverse would be more difficult). 


